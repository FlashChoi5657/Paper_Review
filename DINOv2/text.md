## DINOv2 : Learning Robust Visual Features without Supervision

### Introduction
- 특정 task에 종속되지 않는 사전학습 표현을 학습하는 것은 NLP의 표준이 되었다. 이 학습 방식은 fine-tuning 없이 feature들을 '있는 그대로' 사용할 수 있고 downstream task에서 task-specific 모델보다 더 우수한 성능을 달성할 수 있다. 이러한 성공은 언어 모델링 또는 워드 벡터와 같은 감독이 필요 없는 pretext objetivis를 사용한 대량의 raw text로 사전학습함으로써 이루어졌다.
- NLP의 이러한 패러다임 전환처럼 CV에서도 유사한 foundation 모델의 출현을 기대한다. foundation 모델이라 함은 이미지의 분류 또는 분할 task에서 별도 설정없이 바로 동작하여 시각적 feature를 생성할 수 있어야 한다. 이 분야에서 성공이 가장 기대되는 방식은 텍스트가 보조하는 사전학습(text-guided pretraining)이다. 그러나 형태의 텍스트 보조 사전학습 형식은 caption이 이미지의 풍부한 정보를 대략적으로 표현하고 복잡한 pixel-level 정보는 이러한 supervision에서 드러나지 않게 만들어 이미지에서 보존될 수 있는 정보를 제한한다(CLIP 같은 방식이 caption이 고양이가 소파에 앉아있다 정도의 high level 의미만 담고 고양이의 texture, edge 등의 fine-grained detail은 담지 못하는 한계를 지적한다). 이러한 이미지 인코더들은 텍스트와 이미지 뭉치의 정렬도 필요하고 raw data로만 학습 가능한 텍스트 쪽 언어 모델이 갖는 유연성을 제공하지 못한다.
- 텍스트 보조 사전학습을 대체하는 것은 feature를 이미지로만 학습하는 자기지도 학습이다. 이러한 접근은 언어모델의 pretext task와 개념적으로 유사하고 이미지와 pixel level에서 정보를 기반으로 학습한다. 게다가 자기지도 모델로 생성한 feature 출력은 다양하고 유용한 특성을 보여주고 다양한 분야에 적용도 가능하다. 이러한 잠재력에도 불구하고 그 동안은 ImageNet-1K와 같은 small curated 데이터에 대한 사전학습 맥락에서만 이루어졌다. ImageNet-1K 규모의 한계를 넘어서기 위한 시도가 있었으나 정제되지 않은 데이터로 접근했고 feature quality가 하락했다. 이는 우수한 feature를 만들기 위해 데이터의 quality와 diversity를 제어하는 것이 중요하다는 것을 보여준다.
- 논문에서는 자기지도학습이 광범위한 정제 데이터에서 사전학습 된 경우 다목적 visual feature를 만들 잠재력이 있는지 탐색한다. 현존하는 판별형 자기지도 학습 접근 방법인 iBOT과 같은 방식의 이미지와 patch단위에서 feature를 학습하는 방식을 재검토하고 그들의 설계 선택을 더 큰 규모의 데이터셋을 기준으로 재고해 본다. 본 논문의 대부분 기술적 기여는 판별형 자기지도 학습에서 모델과 데이터 크기를 키울 때 학습을 안정화, 가속화 하는 것이다. 이러한 개선은 유사한 방법론보다 2배 더 빠르고 3배 더 적은 메모리를 사용하여 더 큰 batchsize를 적용해 더 긴 학습을 진행할 수 있게 한다.
- 사전학습 데이터를 만들기 위해 광범위한 미정제 데이터에서 필터링과 재조정을 할 수 있는 자동화 라인을 구축했다. 이것은 NLP 파이프라인과 유사한데, 외부의 메타데이터 대신 데이터 유사도를 사용하며 추가적인 수작업이 필요없다. 광범위한 데이터를 수집하면서 특정 카테고리의 이미지가 압도적으로 많기 때문에 이것을 재조정하고 몇몇의 분포에 과적합 되는 것을 피하는 작업이 어려웠다. 논문에서는 naive clustering 접근법이 이 이슈를 합리적으로 해결하는데 합리적으로 잘 동작시킨다. 우리의 접근을 검증하기 위해 작지만 다양한 종류의 1억4천2백만개의 이미지를 수집했다.
- 마지막으로 DINOv2라 명칭한 자체 데이터로 학습한 다양한 ViT 구조 기반의 사전학습된 visual model들을 제공한다. 모든 모델과 코드를 어떤 데이터에도 적용할 수 있도록 배포한다. 그림2에서 요약한 것처럼 DINOv2는 다양한 CV 벤치마크의 이미지와 픽셀 단위에서 모델을 스케일링 할 수록 훌륭한 성능을 보이는 것을 검증했다. 자기지도 학습에서는 크게 앞서고 약지도 학습과는 근사하게 경쟁할 수 있는 transfer 가능한 고정 feature를 학습하기에 좋은 후보라 결론 짓는다.

### Related Work
- Intra-image self-supervised training(단일 이미지에서 생성한 신호를 이용한 자기지도학습): 자기지도학습 방법의 첫 번째 시도는 이미지로 만들어진 pretext task에 초점을 맞춘다. 즉, 이미지로 학습 신호를 추출하고 이를 이미지의 나머지 부분으로부터 예측하도록 학습한다(이미지의 일부를 맞춰보는 task). Doersch등은 특정 패치의 문맥을 예측하는 방식으로 모델을 학습시켰다. 이후 이미지 re-colorizing, 변환 예측, inpainting 또는 패치 재정렬 등 다양한 pretext task가 제안되었다. 최근에는 ViT와 같은 patch 기반 아키텍처의 등장으로 사전학습 단계에서 inpainting을 다시 활용하는 연구가 활발해졌고 경우에 따라 pixel space가 아닌 feature space에서 수행되기도 한다. 특히, he 등은 MAE가 downstream task에 대해 fine-tuning할 경우 상당한 성능 향상을 제공하는 특징 표현을 학습한다는 것을 보였다. 이러한 MAE의 특성은 비디오, 오디오, 또는 다양한 modality에서 검증되었다. 하지만 이러한 방법들이 학습한 특징들은 supervised learning 기반 fine-tuning이 필요하며 우리 방식은 별도의 fine-tuning 없이 우수한 성능을 보인다.
