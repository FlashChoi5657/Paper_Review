## DINOv2 : Learning Robust Visual Features without Supervision

### Introduction
- 특정 task에 종속되지 않는 사전학습 표현을 학습하는 것은 NLP의 표준이 되었다. 이 학습 방식은 fine-tuning 없이 feature들을 '있는 그대로' 사용할 수 있고 downstream task에서 task-specific 모델보다 더 우수한 성능을 달성할 수 있다. 이러한 성공은 언어 모델링 또는 워드 벡터와 같은 감독이 필요 없는 pretext objetivis를 사용한 대량의 raw text로 사전학습함으로써 이루어졌다.
- NLP의 이러한 패러다임 전환처럼 CV에서도 유사한 foundation 모델의 출현을 기대한다. foundation 모델이라 함은 이미지의 분류 또는 분할 task에서 별도 설정없이 바로 동작하여 시각적 feature를 생성할 수 있어야 한다. 이 분야에서 성공이 가장 기대되는 방식은 텍스트가 보조하는 사전학습(text-guided pretraining)이다. 그러나 형태의 텍스트 보조 사전학습 형식은 caption이 이미지의 풍부한 정보를 대략적으로 표현하고 복잡한 pixel-level 정보는 이러한 supervision에서 드러나지 않게 만들어 이미지에서 보존될 수 있는 정보를 제한한다(CLIP 같은 방식이 caption이 고양이가 소파에 앉아있다 정도의 high level 의미만 담고 고양이의 texture, edge 등의 fine-grained detail은 담지 못하는 한계를 지적한다). 이러한 이미지 인코더들은 텍스트와 이미지 뭉치의 정렬도 필요하고 raw data로만 학습 가능한 텍스트 쪽 언어 모델이 갖는 유연성을 제공하지 못한다.
- 텍스트 보조 사전학습을 대체하는 것은 feature를 이미지로만 학습하는 자기지도 학습이다. 이러한 접근은 언어모델의 pretext task와 개념적으로 유사하고 이미지와 pixel level에서 정보를 기반으로 학습한다. 게다가 자기지도 모델로 생성한 feature 출력은 다양하고 유용한 특성을 보여주고 다양한 분야에 적용도 가능하다. 이러한 잠재력에도 불구하고 그 동안은 ImageNet-1K와 같은 small curated 데이터에 대한 사전학습 맥락에서만 이루어졌다. ImageNet-1K 규모의 한계를 넘어서기 위한 시도가 있었으나 정제되지 않은 데이터로 접근했고 feature quality가 하락했다. 이는 우수한 feature를 만들기 위해 데이터의 quality와 diversity를 제어하는 것이 중요하다는 것을 보여준다.
- 논문에서는 자기지도학습이 광범위한 정제 데이터에서 사전학습 된 경우 다목적 visual feature를 만들 잠재력이 있는지 탐색한다. 현존하는 판별형 자기지도 학습 접근 방법인 iBOT과 같은 방식의 이미지와 patch단위에서 feature를 학습하는 방식을 재검토하고 그들의 설계 선택을 더 큰 규모의 데이터셋을 기준으로 재고해 본다. 본 논문의 대부분 기술적 기여는 판별형 자기지도 학습에서 모델과 데이터 크기를 키울 때 학습을 안정화, 가속화 하는 것이다. 이러한 개선은 유사한 방법론보다 2배 더 빠르고 3배 더 적은 메모리를 사용하여 더 큰 batchsize를 적용해 더 긴 학습을 진행할 수 있게 한다.
